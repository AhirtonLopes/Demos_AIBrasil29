{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo3_RNN_SpamFilter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhirtonLopes/Demos_AIBrasil29/blob/master/Demo3_RNN_SpamFilter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UH01YRlVNlaA",
        "colab_type": "text"
      },
      "source": [
        "# Demo 3 - Recurrent Neural Networks usando Tensorflow\n",
        "\n",
        "### O presente script implementa uma RNN em TensorFlow para a predição de spam/nao spam em texto.\n",
        "\n",
        "Começamos carregando as bibliotecas necessárias e inicializando um gráfico de computação no TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZUz2q7pNlaB",
        "colab_type": "code",
        "outputId": "6cb8a764-af91-4af7-e013-f7e8733d11da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        }
      },
      "source": [
        "import os\n",
        "import re\n",
        "import io\n",
        "import requests\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from zipfile import ZipFile\n",
        "from tensorflow.python.framework import ops\n",
        "ops.reset_default_graph()\n",
        "\n",
        "# Iniciando um gráfico de computação\n",
        "\n",
        "sess = tf.Session()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUv8xhMnNlaE",
        "colab_type": "text"
      },
      "source": [
        "### 1. Setando os parâmetros de nossa RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35c_MGQiNlaF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parâmetros de nossa RNN\n",
        "\n",
        "epochs = 50\n",
        "batch_size = 250\n",
        "max_sequence_length = 25\n",
        "rnn_size = 10\n",
        "embedding_size = 50\n",
        "min_word_frequency = 10\n",
        "learning_rate = 0.0005\n",
        "dropout_keep_prob = tf.placeholder(tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqI1dxuCNlaH",
        "colab_type": "text"
      },
      "source": [
        "### 2. Carregando os dados de treinamento (textos) quanto a SPAM\n",
        "\n",
        "Nós baixamos e salvamos os dados em seguida. Primeiro, verificamos se já o salvamos antes e carregamos localmente, se não, nós o carregamos da internet (repositório da UCI - http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESRF4rbiNlaI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download ou abertura de arquivos referentes a SPAM\n",
        "\n",
        "data_dir = 'temp'\n",
        "data_file = 'text_data.txt'\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "\n",
        "if not os.path.isfile(os.path.join(data_dir, data_file)):\n",
        "    zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
        "    r = requests.get(zip_url)\n",
        "    z = ZipFile(io.BytesIO(r.content))\n",
        "    file = z.read('SMSSpamCollection')\n",
        "    \n",
        "    # Formatando dados\n",
        "    \n",
        "    text_data = file.decode()\n",
        "    text_data = text_data.encode('ascii', errors='ignore')\n",
        "    text_data = text_data.decode().split('\\n')\n",
        "\n",
        "    # Salvando dados em arquivo texto\n",
        "    \n",
        "    with open(os.path.join(data_dir, data_file), 'w') as file_conn:\n",
        "        for text in text_data:\n",
        "            file_conn.write(\"{}\\n\".format(text))\n",
        "else:\n",
        "    \n",
        "    # Recuperando dados de acordo com arquivo texto\n",
        "    \n",
        "    text_data = []\n",
        "    with open(os.path.join(data_dir, data_file), 'r') as file_conn:\n",
        "        for row in file_conn:\n",
        "            text_data.append(row)\n",
        "    text_data = text_data[:-1]\n",
        "\n",
        "text_data = [x.split('\\t') for x in text_data if len(x) >= 1]\n",
        "[text_data_target, text_data_train] = [list(x) for x in zip(*text_data)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRC4zBTwNlaJ",
        "colab_type": "text"
      },
      "source": [
        "### 3. Processamento dos textos e criação de representações numéricas (palavras em índices)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlZRXMiFNlaK",
        "colab_type": "code",
        "outputId": "dedb80ff-63dc-4eb0-d678-533c23bc59bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "source": [
        "# Criando uma função de \"limpeza\"de texto\n",
        "\n",
        "def clean_text(text_string):\n",
        "    text_string = re.sub(r'([^\\s\\w]|_|[0-9])+', '', text_string)\n",
        "    text_string = \" \".join(text_string.split())\n",
        "    text_string = text_string.lower()\n",
        "    return text_string\n",
        "\n",
        "\n",
        "# Textos \"limpos\"\n",
        "\n",
        "text_data_train = [clean_text(x) for x in text_data_train]\n",
        "\n",
        "# Transformando textos em vetores numéricos\n",
        "\n",
        "vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_sequence_length,\n",
        "                                                                     min_frequency=min_word_frequency)\n",
        "text_processed = np.array(list(vocab_processor.fit_transform(text_data_train)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-4-52900158d660>:16: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_hRoFy-NlaM",
        "colab_type": "text"
      },
      "source": [
        "### 4. Divisão dos dados em datasets de treino/teste (80/20%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zidxbGyNlaM",
        "colab_type": "code",
        "outputId": "08bf7197-bcca-4cb2-a920-e193f3f9fda8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Dividindo nossos dados\n",
        "\n",
        "text_processed = np.array(text_processed)\n",
        "text_data_target = np.array([1 if x == 'ham' else 0 for x in text_data_target])\n",
        "shuffled_ix = np.random.permutation(np.arange(len(text_data_target)))\n",
        "x_shuffled = text_processed[shuffled_ix]\n",
        "y_shuffled = text_data_target[shuffled_ix]\n",
        "\n",
        "# Dividindo em datasets de treino/teste \n",
        "\n",
        "ix_cutoff = int(len(y_shuffled)*0.80)\n",
        "x_train, x_test = x_shuffled[:ix_cutoff], x_shuffled[ix_cutoff:]\n",
        "y_train, y_test = y_shuffled[:ix_cutoff], y_shuffled[ix_cutoff:]\n",
        "vocab_size = len(vocab_processor.vocabulary_)\n",
        "print(\"Tamanho do vocabulario: {:d}\".format(vocab_size))\n",
        "print(\"80-20 Divisao em Treino/Teste: {:d} -- {:d}\".format(len(y_train), len(y_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tamanho do vocabulario: 933\n",
            "80-20 Divisao em Treino/Teste: 4459 -- 1115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59VJnLx_NlaQ",
        "colab_type": "text"
      },
      "source": [
        "## 5. Definição de nosso modelo RNN\n",
        "\n",
        "Criamos os espaços reservados para os dados, as matrizes de incorporação de palavras (e as pesquisas de incorporação) e definimos o restante do modelo. \n",
        "\n",
        "O restante do modelo RNN criará uma célula RNN dinâmica (tipo RNN regular), que variará o número de RNNs necessários para o comprimento de entrada variável (quantidade diferente de palavras para textos de entrada) e, em seguida, será enviada para uma camada logística totalmente conectada tendo em vista prever spam ou nao-spam como saída."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS0zXPKtNlaQ",
        "colab_type": "code",
        "outputId": "086f9292-d823-4f4d-b01d-f24399f1c038",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# Criar espaços reservados\n",
        "\n",
        "x_data = tf.placeholder(tf.int32, [None, max_sequence_length])\n",
        "y_output = tf.placeholder(tf.int32, [None])\n",
        "\n",
        "# Criar embedding\n",
        "\n",
        "embedding_mat = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))\n",
        "embedding_output = tf.nn.embedding_lookup(embedding_mat, x_data)\n",
        "\n",
        "# Definir a célula RNN\n",
        "# tensorflow change> = 1.0, o rnn é colocado no diretório tensorflow.contrib.\n",
        "\n",
        "if tf.__version__[0] >= '1':\n",
        "    cell = tf.contrib.rnn.BasicRNNCell(num_units=rnn_size)\n",
        "else:\n",
        "    cell = tf.nn.rnn_cell.BasicRNNCell(num_units=rnn_size)\n",
        "\n",
        "output, state = tf.nn.dynamic_rnn(cell, embedding_output, dtype=tf.float32)\n",
        "output = tf.nn.dropout(output, dropout_keep_prob)\n",
        "\n",
        "# Obter saída da sequência RNN\n",
        "\n",
        "output = tf.transpose(output, [1, 0, 2])\n",
        "last = tf.gather(output, int(output.get_shape()[0]) - 1)\n",
        "\n",
        "weight = tf.Variable(tf.truncated_normal([rnn_size, 2], stddev=0.1))\n",
        "bias = tf.Variable(tf.constant(0.1, shape=[2]))\n",
        "logits_out = tf.matmul(last, weight) + bias"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-e69af7b4d5b3>:14: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-6-e69af7b4d5b3>:18: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:456: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:460: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From <ipython-input-6-e69af7b4d5b3>:19: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXmCSWubNlaS",
        "colab_type": "text"
      },
      "source": [
        "## 6. Declarando a função de perda (entropia cruzada - softmax), uma função de precisão e função de otimização (RMSProp)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i27q_ewxNlaT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Função de perda\n",
        "\n",
        "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_out, labels=y_output)\n",
        "loss = tf.reduce_mean(losses)\n",
        "\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits_out, 1), tf.cast(y_output, tf.int64)), tf.float32))\n",
        "\n",
        "optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
        "train_step = optimizer.minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pRR2KsENlaW",
        "colab_type": "text"
      },
      "source": [
        "Em seguida, inicializamos as variáveis no gráfico computacional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHqz-6YQNlaW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init = tf.global_variables_initializer()\n",
        "sess.run(init)\n",
        "\n",
        "train_loss = []\n",
        "test_loss = []\n",
        "train_accuracy = []\n",
        "test_accuracy = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zqo4kcb0NlaZ",
        "colab_type": "text"
      },
      "source": [
        "## 7. Etapa de treinamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QM0Mh_DvNlaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Iniciando o treinamento\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # \"Embaralhando\"os dados de treinamento\n",
        "\n",
        "    shuffled_ix = np.random.permutation(np.arange(len(x_train)))\n",
        "    x_train = x_train[shuffled_ix]\n",
        "    y_train = y_train[shuffled_ix]\n",
        "    num_batches = int(len(x_train)/batch_size) + 1\n",
        "        \n",
        "    for i in range(num_batches):\n",
        "        \n",
        "        # Selecionando dados de treinamento\n",
        "        \n",
        "        min_ix = i * batch_size\n",
        "        max_ix = np.min([len(x_train), ((i+1) * batch_size)])\n",
        "        x_train_batch = x_train[min_ix:max_ix]\n",
        "        y_train_batch = y_train[min_ix:max_ix]\n",
        "        \n",
        "        # Executando etapa de treinamento\n",
        "        \n",
        "        train_dict = {x_data: x_train_batch, y_output: y_train_batch, dropout_keep_prob:0.5}\n",
        "        sess.run(train_step, feed_dict=train_dict)\n",
        "        \n",
        "    # Executando perda e acuracia para treinamento\n",
        "    \n",
        "    temp_train_loss, temp_train_acc = sess.run([loss, accuracy], feed_dict=train_dict)\n",
        "    train_loss.append(temp_train_loss)\n",
        "    train_accuracy.append(temp_train_acc)\n",
        "    \n",
        "    # Executando etapa de avaliacao (eval. step)\n",
        "    \n",
        "    test_dict = {x_data: x_test, y_output: y_test, dropout_keep_prob:1.0}\n",
        "    temp_test_loss, temp_test_acc = sess.run([loss, accuracy], feed_dict=test_dict)\n",
        "    test_loss.append(temp_test_loss)\n",
        "    test_accuracy.append(temp_test_acc)\n",
        "    \n",
        "    print('Epoca: {}, Perda Teste: {:.2}, Acuracia Teste: {:.2}'.format(epoch+1, temp_test_loss, temp_test_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUKyVK-DNlab",
        "colab_type": "text"
      },
      "source": [
        "## 7. Plots de perdas e acuracia no tempo (geracoes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZzkymRoNlac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# Plot de perdas no tempo\n",
        "\n",
        "epoch_seq = np.arange(1, epochs+1)\n",
        "plt.plot(epoch_seq, train_loss, 'k--', label='Dataset de Treinamento')\n",
        "plt.plot(epoch_seq, test_loss, 'r-', label='Dataset de Teste')\n",
        "plt.title('Perda Softmax')\n",
        "plt.xlabel('Epocas')\n",
        "plt.ylabel('Perda Softmax')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot ada acuracia no tempo\n",
        "\n",
        "plt.plot(epoch_seq, train_accuracy, 'k--', label='Dataset de Treinamento')\n",
        "plt.plot(epoch_seq, test_accuracy, 'r-', label='Dataset de Teste')\n",
        "plt.title('Acuracia de Teste')\n",
        "plt.xlabel('Epocas')\n",
        "plt.ylabel('Acuracia')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNm8PdTjNlaf",
        "colab_type": "text"
      },
      "source": [
        "## 8. Avaliando novos textos (Modifique aqui com textos em inglês de spam/nao spam)\n",
        "\n",
        "Aqui pode-se utilizar o modelo treinado para avaliar novos textos (que podem ou não ser spam)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZcaGafSNlag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_texts = ['Hi, please respond 1111 asap to claim your change to win now!',\n",
        "                'Hey what are you doing for dinner tonight?',\n",
        "                'New offer, show this text for 50% off of our inagural sale! Respond ASAP to claim you reward!!!',\n",
        "                'Can you take the dog to the vet tomorrow?',\n",
        "                'Congratulations! You have been randomly selected to receive account credit! Call 555-666 right now!',\n",
        "                'We are enjoying a lot of great content here at Developers BR meetup']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sIqkOlhNlah",
        "colab_type": "text"
      },
      "source": [
        "Limpeza dos textos da amostra."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUCpyvCQNlai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_texts = [clean_text(text) for text in sample_texts]\n",
        "print(clean_texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86cyECHZNlal",
        "colab_type": "text"
      },
      "source": [
        "Em seguida, transformamos cada texto como uma seqüência de palavras em uma sequência de índices de vocabulário."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLh3nm-WNlal",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processed_texts = np.array(list(vocab_processor.transform(clean_texts)))\n",
        "print(processed_texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZlwTRSpNlao",
        "colab_type": "text"
      },
      "source": [
        "Agora podemos executar cada um dos textos através do nosso modelo e obter saída."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fC0GMQYNlap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_feed_dict = {x_data: processed_texts, dropout_keep_prob: 1.0}\n",
        "model_results = sess.run(tf.nn.softmax(logits_out), feed_dict=eval_feed_dict)\n",
        "\n",
        "print(model_results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6MmJSiLNlas",
        "colab_type": "text"
      },
      "source": [
        "Imprimindo resultados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CBcIatYNlat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categories = ['spam', 'nao spam']\n",
        "\n",
        "for ix, result in enumerate(model_results):\n",
        "    prediction = categories[np.argmax(result)]\n",
        "    \n",
        "    print('Texto: {}, \\nPredicao: {}\\n'.format(sample_texts[ix], prediction))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}